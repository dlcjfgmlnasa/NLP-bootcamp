# NLP bootcamp - Transformer Is All You Need (4th, 19.07.06 ~ 19.09.28)
모두의연구소 flipped school 과정 중 하나인 NLP bootcamp에서 발표한 paper 목록과 그 자료들입니다.

* participant : 
* faciliator : 김보섭

## Schedule
### Week01 (19/07/06)
* Orientation
* Attention Is All You Need
	+ Presenter : 김보섭
	+ Paper :  https://arxiv.org/abs/1408.5882
	+ Material : [Attention Is All You Need_김보섭.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week01/Attention%20Is%20All%20You%20Need_%EA%B9%80%EB%B3%B4%EC%84%AD.pdf)
### Week02 (19/07/13)
* Universal Sentence Encoder
	+ Presenter : 최태균
	+ Paper : https://arxiv.org/abs/1803.11175
	+ Material : [Universal Sentence Encoder_최태균.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week02/Universal%20Sentence%20Encoder_%EC%B5%9C%ED%83%9C%EA%B7%A0.pdf)
* Self-Attention with Relative Position Representations
	+ Presenter : 김수원
	+ Paper : https://arxiv.org/abs/1803.02155
	+ Material : [Self-Attetnion with Relative Position Representations_김수원](https://www.notion.so/Self-Attention-with-Relative-Position-Representations-5cba7d03ceb04555a2b147c06f024671)
### Week03 (19/07/20)
* Neural Machine Translation of Rare Words with Subword Units
	+ Presenter : 유원준
	+ Paper : https://arxiv.org/abs/1508.07909
	+ Material : [Neural Machine Translation of Rare Words with Subword Units_유원준.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week03/Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subword%20Units_%EC%9C%A0%EC%9B%90%EC%A4%80.pdf)
* Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates
	+ Presenter : 이현경
	+ Paper : https://arxiv.org/abs/1804.10959
	+ Material : [Subword Regularization_이현경.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week03/Subword%20Regularization_%EC%9D%B4%ED%98%84%EA%B2%BD.pdf)
### Week04 (19/07/27)
* Improving Language Understanding by Generative Pre-Training
	+ Presenter : 박지민
	+ Paper : https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
	+ Material : 
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 
	+ Presenter : 김민수
	+ Paper : https://arxiv.org/abs/1810.04805
	+ Material :
### Week05 (19/08/10)
* Multi-Task Deep Neural Networks for Natural Language Understanding
	+ Presenter : 김성운
	+ Paper : https://arxiv.org/abs/1901.11504
	+ Material : 
* Language Models are Unsupervised Multitask Learners 
	+ Presenter : 김영
	+ Paper :https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
	+ Material :
### Week06 (19/08/17)
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context 
	+ Presenter : 
	+ Paper :https://arxiv.org/abs/1901.02860
	+ Material : 
* XLNet: Generalized Autoregressive Pretraining for Language Understanding
	+ Presenter :
	+ Paper : https://arxiv.org/abs/1906.08237
	+ Material :
### Week07 (19/08/24)
* BERT for Joint Intent Classification and Slot Filling
	+ Presenter : 
	+ Paper : https://arxiv.org/abs/1902.10909
	+ Material : 
* BERT Baseline for the Natural Questions
	+ Presenter :
	+ Paper : https://arxiv.org/abs/1901.08634
	+ Material :
### Week08 (19/08/31)
* Fine-tune BERT for Extractive Summarization
	+ Presenter : 
	+ Paper : https://arxiv.org/abs/1903.10318
	+ Material : 
* BERT Rediscovers the Classical NLP Pipeline
	+ Presenter :
	+ Paper : https://arxiv.org/pdf/1905.05950.pdf
	+ Material :
### Week09 (19/09/07)
* TBD
### Week10 (19/09/21)
* TBD
### Week11 (19/09/28)
* TBD
