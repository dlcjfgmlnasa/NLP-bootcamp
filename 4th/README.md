# NLP bootcamp - Transformer Is All You Need (4th, 19.07.06 ~ 19.09.28)
모두의연구소 flipped school 과정 중 하나인 NLP bootcamp에서 발표한 paper 목록과 그 자료들입니다.

* participant : 
* faciliator : 김보섭

## Schedule
### Week01 (19/07/06)
* Orientation
* Attention Is All You Need
	+ Presenter : 김보섭
	+ Paper :  https://arxiv.org/abs/1408.5882
	+ Material : [Attention Is All You Need_김보섭.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week01/Attention%20Is%20All%20You%20Need_%EA%B9%80%EB%B3%B4%EC%84%AD.pdf)
### Week02 (19/07/13)
* Universal Sentence Encoder
	+ Presenter : 최태균
	+ Paper : https://arxiv.org/abs/1803.11175
	+ Material : [Universal Sentence Encoder_최태균.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week02/Universal%20Sentence%20Encoder_%EC%B5%9C%ED%83%9C%EA%B7%A0.pdf)
* Self-Attention with Relative Position Representations
	+ Presenter : 김수원
	+ Paper : https://arxiv.org/abs/1803.02155
	+ Material : [Self-Attetnion with Relative Position Representations_김수원](https://www.notion.so/Self-Attention-with-Relative-Position-Representations-5cba7d03ceb04555a2b147c06f024671)
### Week03 (19/07/20)
* Neural Machine Translation of Rare Words with Subword Units
	+ Presenter : 유원준
	+ Paper : https://arxiv.org/abs/1508.07909
	+ Material : [Neural Machine Translation of Rare Words with Subword Units_유원준.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week03/Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subword%20Units_%EC%9C%A0%EC%9B%90%EC%A4%80.pdf)
* Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates
	+ Presenter : 이현경
	+ Paper : https://arxiv.org/abs/1804.10959
	+ Material : [Subword Regularization_이현경.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week03/Subword%20Regularization_%EC%9D%B4%ED%98%84%EA%B2%BD.pdf)
### Week04 (19/07/27)
* Improving Language Understanding by Generative Pre-Training
	+ Presenter : 박지민
	+ Paper : https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
	+ Material : [Improving Language Understanding by Generative Pre-Training_박지민.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week04/Improving%20Language%20Understanding%20by%20Generative%20Pre-Training_%EB%B0%95%EC%A7%80%EB%AF%BC.pdf)
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 
	+ Presenter : 김민수
	+ Paper : https://arxiv.org/abs/1810.04805
	+ Material : [BERT_Pre-training of Deep Bidirectional Transformers for Language Understanding_김민수.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week04/BERT_Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding_%EA%B9%80%EB%AF%BC%EC%88%98.pdf)
### Week05 (19/08/10)
* Multi-Task Deep Neural Networks for Natural Language Understanding
	+ Presenter : 김성운
	+ Paper : https://arxiv.org/abs/1901.11504
	+ Material : [Multi-Task Deep Neural Networks for Natural Language Understanding_김성운.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week05/Multi-Task%20Deep%20Neural%20Networks%20for%20Natural%20Language%20Understanding_%EA%B9%80%EC%84%B1%EC%9A%B4.pdf)
* Language Models are Unsupervised Multitask Learners 
	+ Presenter : 김영
	+ Paper :https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
	+ Material : [Language Models are Unsupervised Multitask Learners_김영.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week05/Language%20Models%20are%20Unsupervised%20Multitask%20Learners_%EA%B9%80%EC%98%81.pdf)
### Week06 (19/08/17)
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context 
	+ Presenter : 김재민, 신성진
	+ Paper :https://arxiv.org/abs/1901.02860
	+ Material : [Transformer-XL_Attentive Language Models Beyond a Fixed-Length Context_김재민_신성진.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week06/Transformer-XL_Attentive%20Language%20Models%20Beyond%20a%20Fixed-Length%20Context_%EA%B9%80%EC%9E%AC%EB%AF%BC_%EC%8B%A0%EC%84%B1%EC%A7%84.pdf)
* XLNet: Generalized Autoregressive Pretraining for Language Understanding
	+ Presenter : 조원호, 남재신
	+ Paper : https://arxiv.org/abs/1906.08237
	+ Reference : [7주_2_XLNet_참고자료_PR-175_.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week06/7%EC%A3%BC_2_XLNet_%EC%B0%B8%EA%B3%A0%EC%9E%90%EB%A3%8C_PR-175_.pdf)
### Week07 (19/08/24)
* torchtext, single sentence classification, pairwise text classification (bert)
	+ Presenter : 김보섭
### Week08 (19/08/31) 
* Cross-lingual Language Model Pretraining
	+ Presenter : 백영상
	+ Paper : https://arxiv.org/abs/1901.07291
	+ Material : [Cross-lingual Language Model Pretraining_백영상](https://www.notion.so/XLM-415cbf5838944c979daabea22e1a99c8)
* ERNIE: Enhanced Representation through Knowledge Integration
	+ Presenter : 한지윤
	+ Paper : https://arxiv.org/abs/1904.09223
	+ Material : [ERNIE_Enhanced Representation through Knowledge Integration_한지윤](https://www.notion.so/NLP-bootcamp_4th-2bcdaf2d0a8748b097120354073c32a3)
### Week09 (19/09/07)
* MASS: Masked Sequence to Sequence Pre-training for Language Generation
  - Presenter : 박인호
  - Paper : https://arxiv.org/abs/1905.02450
  - Material : [MASS: Masked Sequence to Sequence Pre-training for Language Generation_박인호.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/4th/week09/MASS:%20Masked%20Sequence%20to%20Sequence%20Pre-training%20for%20Language%20Generation_%EB%B0%95%EC%9D%B8%ED%98%B8.pdf)
* RoBERTa: A Robustly Optimized BERT Pretraining Approach
  - Presenter : 유인제
  - Paper : https://arxiv.org/abs/1907.11692
  - Material : [RoBERTa: A Robustly Optimized BERT Pretraining Approach_유인제](https://www.notion.so/RoBERTa-d4c1635a43004672b6d312136aebcfef)
### Week10 (19/09/21)
* ERNIE 2.0: A Continual Pre-training Framework for Language Understanding
  - Presenter : 김태우
  - Paper : https://arxiv.org/abs/1907.12412
  - Material :
* Unified Language Model Pre-training for Natural Language Understanding and Generation
	- Presenter : 오휘건
	- Paper : https://arxiv.org/abs/1905.03197
	- Material : 
### Week11 (19/09/28)
* Embedding Is All You Need (special lecture)
	- Presenter : 이기창 (ratsgo)
	- Material : 
* Predicting Molecular Properties (special lecture)
	- Presenter : 이영수
	- Material : 
